{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cabamarcos/GANs_VA/blob/main/DCGAN_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copyfile\n",
        "from google.colab import drive\n",
        "import os, sys\n",
        "# GUARDAR FICEHROS EN DRIVE Y LUEGO MONTAR DRIVE EN COLAB Y PONER EL PATH DONDE ESTAN LOS FICHEROS\n",
        "drive.mount('/content/drive')\n",
        "copyfile('/content/drive/My Drive/GANS/archive.zip', './archive.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "rrwDY_3XlMaZ",
        "outputId": "37858bc9-c6d4-485f-e314-d1a99b8080bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./archive.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/My Drive/GANS'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIIrXozdlO4G",
        "outputId": "ec55b5a4-3e73-4808-d0e5-05b6e0fa9b4c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/GANS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copy2('archive.zip', '/content/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bS21-CUPnjMm",
        "outputId": "5490f7f7-bf20-4c21-aca8-4dfad7a6d89a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/archive.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"..\"\n",
        "%cd \"..\"\n",
        "%cd \"..\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOXzFlqAtbIk",
        "outputId": "f26f10b3-6dd6-443a-83b2-9ff0437b763a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n",
            "/content/drive\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset\n",
        "!unzip -q archive.zip\n",
        "# Remove the zip file\n",
        "!rm archive.zip"
      ],
      "metadata": {
        "id": "82O35JSURskZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # input: N x channels_img x 64 x 64\n",
        "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
        "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n"
      ],
      "metadata": {
        "id": "oQlHLNgPnJvN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2prTvVYsmSR8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters etc.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
        "BATCH_SIZE = 128\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 3\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 20\n",
        "FEATURES_DISC = 64\n",
        "FEATURES_GEN = 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU8pMdfvmyXB",
        "outputId": "cb1182db-8b91-4b97-806c-86b38435160b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Z_fm8A-bm3op"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(disc)\n",
        "\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "GT3SWo2ym-Xa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"logs/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/fake\")\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "disc.train()"
      ],
      "metadata": {
        "id": "zCCMsWz9nAsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd50280a-7c44-422e-d6ed-25f07dbb79b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (disc): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2)\n",
              "    (2): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (6): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (real, _) in enumerate(dataloader):\n",
        "        real = real.to(device)\n",
        "        noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
        "        fake = gen(noise)\n",
        "\n",
        "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
        "        disc_real = disc(real).reshape(-1)\n",
        "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
        "        disc_fake = disc(fake.detach()).reshape(-1)\n",
        "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
        "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
        "        disc.zero_grad()\n",
        "        loss_disc.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
        "        output = disc(fake).reshape(-1)\n",
        "        loss_gen = criterion(output, torch.ones_like(output))\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
        "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise)\n",
        "                # take out (up to) 32 examples\n",
        "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
        "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
        "\n",
        "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "\n",
        "            step += 1"
      ],
      "metadata": {
        "id": "Dak0bkOInD_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca5fc20-817f-40af-adda-66454805c11e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/20] Batch 0/1583                   Loss D: 0.6908, loss G: 0.7964\n",
            "Epoch [0/20] Batch 100/1583                   Loss D: 0.1773, loss G: 2.0004\n",
            "Epoch [0/20] Batch 200/1583                   Loss D: 0.4973, loss G: 3.3891\n",
            "Epoch [0/20] Batch 300/1583                   Loss D: 0.5508, loss G: 2.2981\n",
            "Epoch [0/20] Batch 400/1583                   Loss D: 0.5854, loss G: 1.6637\n",
            "Epoch [0/20] Batch 500/1583                   Loss D: 0.5106, loss G: 2.0170\n",
            "Epoch [0/20] Batch 600/1583                   Loss D: 0.5503, loss G: 1.5639\n",
            "Epoch [0/20] Batch 700/1583                   Loss D: 0.4778, loss G: 1.8515\n",
            "Epoch [0/20] Batch 800/1583                   Loss D: 0.5946, loss G: 1.4143\n",
            "Epoch [0/20] Batch 900/1583                   Loss D: 0.4043, loss G: 2.9515\n",
            "Epoch [0/20] Batch 1000/1583                   Loss D: 0.5399, loss G: 1.6551\n",
            "Epoch [0/20] Batch 1100/1583                   Loss D: 0.5263, loss G: 1.2968\n",
            "Epoch [0/20] Batch 1200/1583                   Loss D: 0.4867, loss G: 1.5042\n",
            "Epoch [0/20] Batch 1300/1583                   Loss D: 0.5286, loss G: 1.4670\n",
            "Epoch [0/20] Batch 1400/1583                   Loss D: 0.7483, loss G: 2.7088\n",
            "Epoch [0/20] Batch 1500/1583                   Loss D: 0.5893, loss G: 1.5477\n",
            "Epoch [1/20] Batch 0/1583                   Loss D: 0.4638, loss G: 2.0465\n",
            "Epoch [1/20] Batch 100/1583                   Loss D: 0.4187, loss G: 2.0940\n",
            "Epoch [1/20] Batch 200/1583                   Loss D: 0.5012, loss G: 3.1236\n",
            "Epoch [1/20] Batch 300/1583                   Loss D: 0.4449, loss G: 2.0426\n",
            "Epoch [1/20] Batch 400/1583                   Loss D: 0.4978, loss G: 1.8321\n",
            "Epoch [1/20] Batch 500/1583                   Loss D: 0.7310, loss G: 4.4941\n",
            "Epoch [1/20] Batch 600/1583                   Loss D: 0.4136, loss G: 1.8490\n",
            "Epoch [1/20] Batch 700/1583                   Loss D: 0.4531, loss G: 1.3558\n",
            "Epoch [1/20] Batch 800/1583                   Loss D: 0.4468, loss G: 1.2450\n",
            "Epoch [1/20] Batch 900/1583                   Loss D: 0.5133, loss G: 2.7046\n",
            "Epoch [1/20] Batch 1000/1583                   Loss D: 0.4652, loss G: 1.4298\n",
            "Epoch [1/20] Batch 1100/1583                   Loss D: 0.5751, loss G: 2.4176\n",
            "Epoch [1/20] Batch 1200/1583                   Loss D: 0.5357, loss G: 1.5719\n",
            "Epoch [1/20] Batch 1300/1583                   Loss D: 0.4780, loss G: 1.4599\n",
            "Epoch [1/20] Batch 1400/1583                   Loss D: 0.4933, loss G: 0.9404\n",
            "Epoch [1/20] Batch 1500/1583                   Loss D: 0.5028, loss G: 1.9408\n",
            "Epoch [2/20] Batch 0/1583                   Loss D: 0.4694, loss G: 1.8532\n",
            "Epoch [2/20] Batch 100/1583                   Loss D: 0.4412, loss G: 2.0661\n",
            "Epoch [2/20] Batch 200/1583                   Loss D: 0.4903, loss G: 2.5001\n",
            "Epoch [2/20] Batch 300/1583                   Loss D: 0.3881, loss G: 1.6471\n",
            "Epoch [2/20] Batch 400/1583                   Loss D: 0.4487, loss G: 1.7452\n",
            "Epoch [2/20] Batch 500/1583                   Loss D: 0.6926, loss G: 3.0363\n",
            "Epoch [2/20] Batch 600/1583                   Loss D: 0.4630, loss G: 2.0298\n",
            "Epoch [2/20] Batch 700/1583                   Loss D: 0.6109, loss G: 1.5197\n",
            "Epoch [2/20] Batch 800/1583                   Loss D: 0.4617, loss G: 1.2894\n",
            "Epoch [2/20] Batch 900/1583                   Loss D: 0.4751, loss G: 1.1659\n",
            "Epoch [2/20] Batch 1000/1583                   Loss D: 0.5469, loss G: 2.2328\n",
            "Epoch [2/20] Batch 1100/1583                   Loss D: 0.4786, loss G: 1.5000\n",
            "Epoch [2/20] Batch 1200/1583                   Loss D: 0.5554, loss G: 1.1499\n",
            "Epoch [2/20] Batch 1300/1583                   Loss D: 0.5249, loss G: 1.7625\n",
            "Epoch [2/20] Batch 1400/1583                   Loss D: 0.5375, loss G: 1.8442\n",
            "Epoch [2/20] Batch 1500/1583                   Loss D: 0.5321, loss G: 1.1541\n",
            "Epoch [3/20] Batch 0/1583                   Loss D: 0.4976, loss G: 1.0044\n",
            "Epoch [3/20] Batch 100/1583                   Loss D: 0.4994, loss G: 1.6708\n",
            "Epoch [3/20] Batch 200/1583                   Loss D: 0.4629, loss G: 1.8770\n",
            "Epoch [3/20] Batch 300/1583                   Loss D: 0.3707, loss G: 1.7296\n",
            "Epoch [3/20] Batch 400/1583                   Loss D: 0.5805, loss G: 1.4302\n",
            "Epoch [3/20] Batch 500/1583                   Loss D: 0.4821, loss G: 1.9946\n",
            "Epoch [3/20] Batch 600/1583                   Loss D: 0.5176, loss G: 1.1571\n",
            "Epoch [3/20] Batch 700/1583                   Loss D: 0.4089, loss G: 1.3691\n",
            "Epoch [3/20] Batch 800/1583                   Loss D: 0.7821, loss G: 0.3630\n",
            "Epoch [3/20] Batch 900/1583                   Loss D: 0.4864, loss G: 1.8126\n",
            "Epoch [3/20] Batch 1000/1583                   Loss D: 0.5017, loss G: 2.3828\n",
            "Epoch [3/20] Batch 1100/1583                   Loss D: 0.4845, loss G: 1.2384\n",
            "Epoch [3/20] Batch 1200/1583                   Loss D: 0.5196, loss G: 1.4742\n",
            "Epoch [3/20] Batch 1300/1583                   Loss D: 0.4679, loss G: 1.2548\n",
            "Epoch [3/20] Batch 1400/1583                   Loss D: 0.5142, loss G: 1.2032\n",
            "Epoch [3/20] Batch 1500/1583                   Loss D: 0.4415, loss G: 1.3123\n",
            "Epoch [4/20] Batch 0/1583                   Loss D: 0.4249, loss G: 2.1620\n",
            "Epoch [4/20] Batch 100/1583                   Loss D: 0.5345, loss G: 1.0581\n",
            "Epoch [4/20] Batch 200/1583                   Loss D: 0.5186, loss G: 1.9081\n",
            "Epoch [4/20] Batch 300/1583                   Loss D: 0.4955, loss G: 1.3334\n",
            "Epoch [4/20] Batch 400/1583                   Loss D: 0.5822, loss G: 1.7087\n",
            "Epoch [4/20] Batch 500/1583                   Loss D: 0.6438, loss G: 2.5888\n",
            "Epoch [4/20] Batch 600/1583                   Loss D: 0.4207, loss G: 1.5795\n",
            "Epoch [4/20] Batch 700/1583                   Loss D: 0.4100, loss G: 1.8087\n",
            "Epoch [4/20] Batch 800/1583                   Loss D: 0.3935, loss G: 1.3387\n",
            "Epoch [4/20] Batch 900/1583                   Loss D: 0.3895, loss G: 1.9144\n",
            "Epoch [4/20] Batch 1000/1583                   Loss D: 0.8603, loss G: 0.2533\n",
            "Epoch [4/20] Batch 1100/1583                   Loss D: 0.4678, loss G: 1.6512\n",
            "Epoch [4/20] Batch 1200/1583                   Loss D: 0.5380, loss G: 1.2229\n",
            "Epoch [4/20] Batch 1300/1583                   Loss D: 0.4678, loss G: 1.6618\n",
            "Epoch [4/20] Batch 1400/1583                   Loss D: 0.3697, loss G: 1.7788\n",
            "Epoch [4/20] Batch 1500/1583                   Loss D: 1.0201, loss G: 0.6527\n",
            "Epoch [5/20] Batch 0/1583                   Loss D: 0.4734, loss G: 1.2395\n",
            "Epoch [5/20] Batch 100/1583                   Loss D: 0.4603, loss G: 1.0979\n",
            "Epoch [5/20] Batch 200/1583                   Loss D: 0.4994, loss G: 1.3020\n",
            "Epoch [5/20] Batch 300/1583                   Loss D: 0.5109, loss G: 1.0647\n",
            "Epoch [5/20] Batch 400/1583                   Loss D: 0.4286, loss G: 1.4865\n",
            "Epoch [5/20] Batch 500/1583                   Loss D: 0.5010, loss G: 1.2464\n",
            "Epoch [5/20] Batch 600/1583                   Loss D: 0.4979, loss G: 2.2063\n",
            "Epoch [5/20] Batch 700/1583                   Loss D: 0.4252, loss G: 1.8117\n",
            "Epoch [5/20] Batch 800/1583                   Loss D: 0.4973, loss G: 0.9589\n",
            "Epoch [5/20] Batch 900/1583                   Loss D: 0.4999, loss G: 0.9199\n",
            "Epoch [5/20] Batch 1000/1583                   Loss D: 0.4781, loss G: 3.0100\n",
            "Epoch [5/20] Batch 1100/1583                   Loss D: 0.4170, loss G: 1.3048\n",
            "Epoch [5/20] Batch 1200/1583                   Loss D: 0.4545, loss G: 1.7917\n",
            "Epoch [5/20] Batch 1300/1583                   Loss D: 0.4087, loss G: 1.6323\n",
            "Epoch [5/20] Batch 1400/1583                   Loss D: 0.4361, loss G: 2.1441\n",
            "Epoch [5/20] Batch 1500/1583                   Loss D: 0.5007, loss G: 2.7694\n",
            "Epoch [6/20] Batch 0/1583                   Loss D: 0.4131, loss G: 2.2216\n",
            "Epoch [6/20] Batch 100/1583                   Loss D: 0.5314, loss G: 3.3171\n",
            "Epoch [6/20] Batch 200/1583                   Loss D: 0.2969, loss G: 2.6190\n",
            "Epoch [6/20] Batch 300/1583                   Loss D: 0.4304, loss G: 1.3767\n",
            "Epoch [6/20] Batch 400/1583                   Loss D: 0.4391, loss G: 0.9744\n",
            "Epoch [6/20] Batch 500/1583                   Loss D: 0.4171, loss G: 1.1548\n",
            "Epoch [6/20] Batch 600/1583                   Loss D: 0.4125, loss G: 2.2346\n",
            "Epoch [6/20] Batch 700/1583                   Loss D: 0.3562, loss G: 2.0982\n",
            "Epoch [6/20] Batch 800/1583                   Loss D: 0.3733, loss G: 2.2891\n",
            "Epoch [6/20] Batch 900/1583                   Loss D: 0.5540, loss G: 1.2265\n",
            "Epoch [6/20] Batch 1000/1583                   Loss D: 0.3015, loss G: 2.6591\n",
            "Epoch [6/20] Batch 1100/1583                   Loss D: 1.6729, loss G: 6.4379\n",
            "Epoch [6/20] Batch 1200/1583                   Loss D: 0.3422, loss G: 1.7582\n",
            "Epoch [6/20] Batch 1300/1583                   Loss D: 0.4030, loss G: 1.3788\n",
            "Epoch [6/20] Batch 1400/1583                   Loss D: 0.2916, loss G: 2.1226\n",
            "Epoch [6/20] Batch 1500/1583                   Loss D: 0.4734, loss G: 1.1548\n",
            "Epoch [7/20] Batch 0/1583                   Loss D: 0.3608, loss G: 1.4214\n",
            "Epoch [7/20] Batch 100/1583                   Loss D: 0.3222, loss G: 2.6770\n",
            "Epoch [7/20] Batch 200/1583                   Loss D: 0.5115, loss G: 2.9883\n",
            "Epoch [7/20] Batch 300/1583                   Loss D: 0.3125, loss G: 2.3423\n",
            "Epoch [7/20] Batch 400/1583                   Loss D: 0.3385, loss G: 2.2337\n",
            "Epoch [7/20] Batch 500/1583                   Loss D: 0.3573, loss G: 1.4156\n",
            "Epoch [7/20] Batch 600/1583                   Loss D: 0.3556, loss G: 2.0613\n",
            "Epoch [7/20] Batch 700/1583                   Loss D: 0.4557, loss G: 1.3233\n",
            "Epoch [7/20] Batch 800/1583                   Loss D: 0.4106, loss G: 2.0356\n",
            "Epoch [7/20] Batch 900/1583                   Loss D: 0.2756, loss G: 2.3470\n",
            "Epoch [7/20] Batch 1000/1583                   Loss D: 0.2867, loss G: 1.6562\n",
            "Epoch [7/20] Batch 1100/1583                   Loss D: 0.5103, loss G: 3.4362\n",
            "Epoch [7/20] Batch 1200/1583                   Loss D: 0.2864, loss G: 2.3306\n",
            "Epoch [7/20] Batch 1300/1583                   Loss D: 0.4004, loss G: 1.7672\n",
            "Epoch [7/20] Batch 1400/1583                   Loss D: 0.4757, loss G: 3.1343\n",
            "Epoch [7/20] Batch 1500/1583                   Loss D: 0.3862, loss G: 1.5711\n",
            "Epoch [8/20] Batch 0/1583                   Loss D: 0.2450, loss G: 2.4407\n",
            "Epoch [8/20] Batch 100/1583                   Loss D: 0.4220, loss G: 1.2428\n",
            "Epoch [8/20] Batch 200/1583                   Loss D: 0.2679, loss G: 2.1341\n",
            "Epoch [8/20] Batch 300/1583                   Loss D: 0.2620, loss G: 2.9471\n",
            "Epoch [8/20] Batch 400/1583                   Loss D: 0.2971, loss G: 1.6710\n",
            "Epoch [8/20] Batch 500/1583                   Loss D: 0.3384, loss G: 1.5079\n",
            "Epoch [8/20] Batch 600/1583                   Loss D: 0.2751, loss G: 2.4309\n",
            "Epoch [8/20] Batch 700/1583                   Loss D: 0.2558, loss G: 2.2027\n",
            "Epoch [8/20] Batch 800/1583                   Loss D: 0.3230, loss G: 2.4920\n",
            "Epoch [8/20] Batch 900/1583                   Loss D: 0.2446, loss G: 2.5812\n",
            "Epoch [8/20] Batch 1000/1583                   Loss D: 0.3203, loss G: 1.7791\n",
            "Epoch [8/20] Batch 1100/1583                   Loss D: 0.3365, loss G: 1.4295\n",
            "Epoch [8/20] Batch 1200/1583                   Loss D: 0.3453, loss G: 3.3119\n",
            "Epoch [8/20] Batch 1300/1583                   Loss D: 0.3376, loss G: 2.3685\n",
            "Epoch [8/20] Batch 1400/1583                   Loss D: 0.4971, loss G: 4.3107\n",
            "Epoch [8/20] Batch 1500/1583                   Loss D: 0.2839, loss G: 1.7024\n",
            "Epoch [9/20] Batch 0/1583                   Loss D: 0.2963, loss G: 2.3128\n",
            "Epoch [9/20] Batch 100/1583                   Loss D: 0.4352, loss G: 0.7742\n",
            "Epoch [9/20] Batch 200/1583                   Loss D: 0.2676, loss G: 2.7298\n",
            "Epoch [9/20] Batch 300/1583                   Loss D: 0.2646, loss G: 2.9127\n",
            "Epoch [9/20] Batch 400/1583                   Loss D: 0.3031, loss G: 3.0506\n",
            "Epoch [9/20] Batch 500/1583                   Loss D: 0.3198, loss G: 2.0002\n",
            "Epoch [9/20] Batch 600/1583                   Loss D: 0.5460, loss G: 0.2608\n",
            "Epoch [9/20] Batch 700/1583                   Loss D: 0.2392, loss G: 2.6961\n",
            "Epoch [9/20] Batch 800/1583                   Loss D: 0.2355, loss G: 2.4923\n",
            "Epoch [9/20] Batch 900/1583                   Loss D: 0.3021, loss G: 1.8156\n",
            "Epoch [9/20] Batch 1000/1583                   Loss D: 0.2152, loss G: 3.1388\n",
            "Epoch [9/20] Batch 1100/1583                   Loss D: 0.1910, loss G: 2.9158\n",
            "Epoch [9/20] Batch 1200/1583                   Loss D: 0.2198, loss G: 3.0047\n",
            "Epoch [9/20] Batch 1300/1583                   Loss D: 0.2827, loss G: 3.8733\n",
            "Epoch [9/20] Batch 1400/1583                   Loss D: 0.5378, loss G: 1.5957\n",
            "Epoch [9/20] Batch 1500/1583                   Loss D: 0.2819, loss G: 2.0435\n",
            "Epoch [10/20] Batch 0/1583                   Loss D: 0.4537, loss G: 1.4337\n",
            "Epoch [10/20] Batch 100/1583                   Loss D: 0.2445, loss G: 2.2480\n",
            "Epoch [10/20] Batch 200/1583                   Loss D: 0.4769, loss G: 0.7514\n",
            "Epoch [10/20] Batch 300/1583                   Loss D: 0.2015, loss G: 2.5375\n",
            "Epoch [10/20] Batch 400/1583                   Loss D: 0.5249, loss G: 3.6690\n",
            "Epoch [10/20] Batch 500/1583                   Loss D: 0.2314, loss G: 2.1912\n",
            "Epoch [10/20] Batch 600/1583                   Loss D: 0.2856, loss G: 3.6775\n",
            "Epoch [10/20] Batch 700/1583                   Loss D: 0.2156, loss G: 3.3920\n",
            "Epoch [10/20] Batch 800/1583                   Loss D: 0.2384, loss G: 2.3596\n",
            "Epoch [10/20] Batch 900/1583                   Loss D: 0.2465, loss G: 3.6409\n",
            "Epoch [10/20] Batch 1000/1583                   Loss D: 0.2673, loss G: 3.4606\n",
            "Epoch [10/20] Batch 1100/1583                   Loss D: 1.0375, loss G: 3.5592\n",
            "Epoch [10/20] Batch 1200/1583                   Loss D: 0.4203, loss G: 1.1497\n",
            "Epoch [10/20] Batch 1300/1583                   Loss D: 0.7318, loss G: 5.0161\n",
            "Epoch [10/20] Batch 1400/1583                   Loss D: 0.3138, loss G: 4.2649\n",
            "Epoch [10/20] Batch 1500/1583                   Loss D: 0.2319, loss G: 2.7068\n",
            "Epoch [11/20] Batch 0/1583                   Loss D: 0.2484, loss G: 2.7874\n",
            "Epoch [11/20] Batch 100/1583                   Loss D: 0.2024, loss G: 2.7518\n",
            "Epoch [11/20] Batch 200/1583                   Loss D: 0.2062, loss G: 3.1619\n",
            "Epoch [11/20] Batch 300/1583                   Loss D: 0.2868, loss G: 2.7475\n",
            "Epoch [11/20] Batch 400/1583                   Loss D: 0.2782, loss G: 1.8936\n",
            "Epoch [11/20] Batch 500/1583                   Loss D: 0.4080, loss G: 1.2314\n",
            "Epoch [11/20] Batch 600/1583                   Loss D: 0.2182, loss G: 3.4674\n",
            "Epoch [11/20] Batch 700/1583                   Loss D: 0.2736, loss G: 1.2240\n",
            "Epoch [11/20] Batch 800/1583                   Loss D: 0.2468, loss G: 2.9803\n",
            "Epoch [11/20] Batch 900/1583                   Loss D: 0.3924, loss G: 4.1591\n",
            "Epoch [11/20] Batch 1000/1583                   Loss D: 0.1777, loss G: 3.0287\n",
            "Epoch [11/20] Batch 1100/1583                   Loss D: 0.3156, loss G: 0.5062\n",
            "Epoch [11/20] Batch 1200/1583                   Loss D: 0.3278, loss G: 3.7089\n",
            "Epoch [11/20] Batch 1300/1583                   Loss D: 0.2182, loss G: 2.3930\n",
            "Epoch [11/20] Batch 1400/1583                   Loss D: 0.1983, loss G: 2.5555\n",
            "Epoch [11/20] Batch 1500/1583                   Loss D: 0.3702, loss G: 2.4944\n",
            "Epoch [12/20] Batch 0/1583                   Loss D: 0.2292, loss G: 1.8552\n",
            "Epoch [12/20] Batch 100/1583                   Loss D: 0.3496, loss G: 2.0435\n",
            "Epoch [12/20] Batch 200/1583                   Loss D: 0.1791, loss G: 2.0092\n",
            "Epoch [12/20] Batch 300/1583                   Loss D: 0.1994, loss G: 2.1275\n",
            "Epoch [12/20] Batch 400/1583                   Loss D: 0.2219, loss G: 2.9671\n",
            "Epoch [12/20] Batch 500/1583                   Loss D: 0.1984, loss G: 3.0003\n",
            "Epoch [12/20] Batch 600/1583                   Loss D: 0.1666, loss G: 2.7989\n",
            "Epoch [12/20] Batch 700/1583                   Loss D: 0.3022, loss G: 4.0755\n",
            "Epoch [12/20] Batch 800/1583                   Loss D: 0.1893, loss G: 3.9360\n",
            "Epoch [12/20] Batch 900/1583                   Loss D: 0.2111, loss G: 2.4007\n",
            "Epoch [12/20] Batch 1000/1583                   Loss D: 0.6346, loss G: 1.2963\n",
            "Epoch [12/20] Batch 1100/1583                   Loss D: 0.1594, loss G: 2.0676\n",
            "Epoch [12/20] Batch 1200/1583                   Loss D: 0.3686, loss G: 1.8080\n",
            "Epoch [12/20] Batch 1300/1583                   Loss D: 0.2169, loss G: 3.8848\n",
            "Epoch [12/20] Batch 1400/1583                   Loss D: 0.1262, loss G: 2.8779\n",
            "Epoch [12/20] Batch 1500/1583                   Loss D: 0.3184, loss G: 3.0737\n",
            "Epoch [13/20] Batch 0/1583                   Loss D: 0.2787, loss G: 3.9239\n",
            "Epoch [13/20] Batch 100/1583                   Loss D: 0.1696, loss G: 3.1336\n",
            "Epoch [13/20] Batch 200/1583                   Loss D: 0.4625, loss G: 5.2359\n",
            "Epoch [13/20] Batch 300/1583                   Loss D: 0.1848, loss G: 1.9826\n",
            "Epoch [13/20] Batch 400/1583                   Loss D: 0.5628, loss G: 5.2479\n",
            "Epoch [13/20] Batch 500/1583                   Loss D: 0.2048, loss G: 1.8904\n",
            "Epoch [13/20] Batch 600/1583                   Loss D: 0.2194, loss G: 2.5514\n",
            "Epoch [13/20] Batch 700/1583                   Loss D: 0.1619, loss G: 3.0910\n",
            "Epoch [13/20] Batch 800/1583                   Loss D: 0.1843, loss G: 2.6480\n",
            "Epoch [13/20] Batch 900/1583                   Loss D: 0.1887, loss G: 3.9930\n",
            "Epoch [13/20] Batch 1000/1583                   Loss D: 0.1511, loss G: 2.6842\n",
            "Epoch [13/20] Batch 1100/1583                   Loss D: 0.1882, loss G: 2.8671\n",
            "Epoch [13/20] Batch 1200/1583                   Loss D: 0.2524, loss G: 3.5479\n",
            "Epoch [13/20] Batch 1300/1583                   Loss D: 0.1964, loss G: 3.7676\n",
            "Epoch [13/20] Batch 1400/1583                   Loss D: 0.3553, loss G: 1.3198\n",
            "Epoch [13/20] Batch 1500/1583                   Loss D: 0.1664, loss G: 2.8233\n",
            "Epoch [14/20] Batch 0/1583                   Loss D: 0.1050, loss G: 3.6776\n",
            "Epoch [14/20] Batch 100/1583                   Loss D: 0.7606, loss G: 5.2106\n",
            "Epoch [14/20] Batch 200/1583                   Loss D: 0.1535, loss G: 1.9845\n",
            "Epoch [14/20] Batch 300/1583                   Loss D: 0.2221, loss G: 3.5119\n",
            "Epoch [14/20] Batch 400/1583                   Loss D: 0.2051, loss G: 3.6784\n",
            "Epoch [14/20] Batch 500/1583                   Loss D: 0.3541, loss G: 4.3923\n",
            "Epoch [14/20] Batch 600/1583                   Loss D: 0.1398, loss G: 3.0234\n",
            "Epoch [14/20] Batch 700/1583                   Loss D: 0.0487, loss G: 4.9431\n",
            "Epoch [14/20] Batch 800/1583                   Loss D: 0.2156, loss G: 3.6424\n",
            "Epoch [14/20] Batch 900/1583                   Loss D: 0.2166, loss G: 3.4819\n",
            "Epoch [14/20] Batch 1000/1583                   Loss D: 0.2949, loss G: 1.6054\n",
            "Epoch [14/20] Batch 1100/1583                   Loss D: 0.1609, loss G: 3.0733\n",
            "Epoch [14/20] Batch 1200/1583                   Loss D: 0.5933, loss G: 0.8184\n",
            "Epoch [14/20] Batch 1300/1583                   Loss D: 0.1621, loss G: 2.7635\n",
            "Epoch [14/20] Batch 1400/1583                   Loss D: 0.2197, loss G: 4.4062\n",
            "Epoch [14/20] Batch 1500/1583                   Loss D: 0.2309, loss G: 2.8738\n",
            "Epoch [15/20] Batch 0/1583                   Loss D: 0.1084, loss G: 3.2043\n",
            "Epoch [15/20] Batch 100/1583                   Loss D: 0.2015, loss G: 3.5984\n",
            "Epoch [15/20] Batch 300/1583                   Loss D: 0.1522, loss G: 3.1126\n",
            "Epoch [15/20] Batch 400/1583                   Loss D: 0.1330, loss G: 2.9169\n",
            "Epoch [15/20] Batch 500/1583                   Loss D: 0.1375, loss G: 3.5658\n",
            "Epoch [15/20] Batch 600/1583                   Loss D: 0.1481, loss G: 2.9495\n",
            "Epoch [15/20] Batch 700/1583                   Loss D: 0.2612, loss G: 4.4857\n",
            "Epoch [15/20] Batch 800/1583                   Loss D: 0.1321, loss G: 3.5226\n",
            "Epoch [15/20] Batch 900/1583                   Loss D: 0.1085, loss G: 3.4482\n",
            "Epoch [15/20] Batch 1000/1583                   Loss D: 0.9045, loss G: 0.2087\n",
            "Epoch [15/20] Batch 1100/1583                   Loss D: 0.4140, loss G: 0.7078\n",
            "Epoch [15/20] Batch 1200/1583                   Loss D: 0.1341, loss G: 2.4082\n",
            "Epoch [15/20] Batch 1300/1583                   Loss D: 0.1434, loss G: 2.7287\n",
            "Epoch [15/20] Batch 1400/1583                   Loss D: 0.3118, loss G: 1.5071\n",
            "Epoch [15/20] Batch 1500/1583                   Loss D: 0.2029, loss G: 3.3255\n",
            "Epoch [16/20] Batch 0/1583                   Loss D: 0.1092, loss G: 2.8377\n",
            "Epoch [16/20] Batch 100/1583                   Loss D: 0.1430, loss G: 4.0349\n",
            "Epoch [16/20] Batch 200/1583                   Loss D: 0.1098, loss G: 3.5388\n",
            "Epoch [16/20] Batch 300/1583                   Loss D: 0.2318, loss G: 4.9745\n",
            "Epoch [16/20] Batch 400/1583                   Loss D: 0.2381, loss G: 2.8619\n",
            "Epoch [16/20] Batch 500/1583                   Loss D: 0.2410, loss G: 2.5771\n",
            "Epoch [16/20] Batch 600/1583                   Loss D: 0.1275, loss G: 3.1704\n",
            "Epoch [16/20] Batch 700/1583                   Loss D: 2.6616, loss G: 5.2423\n",
            "Epoch [16/20] Batch 800/1583                   Loss D: 0.1612, loss G: 2.0881\n",
            "Epoch [16/20] Batch 900/1583                   Loss D: 0.1365, loss G: 3.3508\n",
            "Epoch [16/20] Batch 1000/1583                   Loss D: 0.1282, loss G: 2.4957\n",
            "Epoch [16/20] Batch 1100/1583                   Loss D: 0.1176, loss G: 3.3893\n",
            "Epoch [16/20] Batch 1200/1583                   Loss D: 0.1186, loss G: 3.3555\n",
            "Epoch [16/20] Batch 1300/1583                   Loss D: 0.1085, loss G: 2.9857\n",
            "Epoch [16/20] Batch 1400/1583                   Loss D: 0.5757, loss G: 6.8734\n",
            "Epoch [16/20] Batch 1500/1583                   Loss D: 0.4022, loss G: 1.8625\n",
            "Epoch [17/20] Batch 0/1583                   Loss D: 0.1299, loss G: 4.3666\n",
            "Epoch [17/20] Batch 100/1583                   Loss D: 0.0898, loss G: 3.8453\n",
            "Epoch [17/20] Batch 200/1583                   Loss D: 0.0924, loss G: 3.2844\n",
            "Epoch [17/20] Batch 300/1583                   Loss D: 0.1497, loss G: 3.0708\n",
            "Epoch [17/20] Batch 400/1583                   Loss D: 0.1146, loss G: 2.7222\n",
            "Epoch [17/20] Batch 500/1583                   Loss D: 0.1416, loss G: 2.9464\n",
            "Epoch [17/20] Batch 600/1583                   Loss D: 0.1285, loss G: 4.3219\n",
            "Epoch [17/20] Batch 700/1583                   Loss D: 0.3102, loss G: 4.3492\n",
            "Epoch [17/20] Batch 800/1583                   Loss D: 0.1304, loss G: 3.0641\n",
            "Epoch [17/20] Batch 900/1583                   Loss D: 0.8370, loss G: 0.2393\n",
            "Epoch [17/20] Batch 1000/1583                   Loss D: 0.1522, loss G: 2.8485\n",
            "Epoch [17/20] Batch 1100/1583                   Loss D: 0.0761, loss G: 2.9780\n",
            "Epoch [17/20] Batch 1200/1583                   Loss D: 0.2124, loss G: 5.0094\n",
            "Epoch [17/20] Batch 1300/1583                   Loss D: 0.1751, loss G: 3.9417\n",
            "Epoch [17/20] Batch 1400/1583                   Loss D: 0.0719, loss G: 3.0680\n",
            "Epoch [17/20] Batch 1500/1583                   Loss D: 0.5881, loss G: 5.4351\n",
            "Epoch [18/20] Batch 0/1583                   Loss D: 0.2592, loss G: 4.6863\n",
            "Epoch [18/20] Batch 100/1583                   Loss D: 0.1258, loss G: 3.6022\n",
            "Epoch [18/20] Batch 200/1583                   Loss D: 0.1390, loss G: 3.5318\n",
            "Epoch [18/20] Batch 300/1583                   Loss D: 0.1370, loss G: 3.2875\n",
            "Epoch [18/20] Batch 400/1583                   Loss D: 0.1187, loss G: 3.3909\n",
            "Epoch [18/20] Batch 500/1583                   Loss D: 0.2302, loss G: 2.6813\n",
            "Epoch [18/20] Batch 600/1583                   Loss D: 0.0890, loss G: 3.2199\n",
            "Epoch [18/20] Batch 700/1583                   Loss D: 0.1034, loss G: 4.2843\n",
            "Epoch [18/20] Batch 800/1583                   Loss D: 0.0931, loss G: 4.3659\n",
            "Epoch [18/20] Batch 900/1583                   Loss D: 0.2370, loss G: 5.1686\n",
            "Epoch [18/20] Batch 1000/1583                   Loss D: 0.3060, loss G: 1.9884\n",
            "Epoch [18/20] Batch 1100/1583                   Loss D: 0.2936, loss G: 1.4654\n",
            "Epoch [18/20] Batch 1200/1583                   Loss D: 0.0928, loss G: 3.4840\n",
            "Epoch [18/20] Batch 1300/1583                   Loss D: 0.1341, loss G: 2.3907\n",
            "Epoch [18/20] Batch 1400/1583                   Loss D: 0.1188, loss G: 3.0543\n",
            "Epoch [18/20] Batch 1500/1583                   Loss D: 0.1119, loss G: 3.2922\n",
            "Epoch [19/20] Batch 0/1583                   Loss D: 0.1908, loss G: 2.0002\n",
            "Epoch [19/20] Batch 100/1583                   Loss D: 0.2743, loss G: 1.6921\n",
            "Epoch [19/20] Batch 200/1583                   Loss D: 0.1192, loss G: 4.1738\n",
            "Epoch [19/20] Batch 300/1583                   Loss D: 0.0980, loss G: 4.5465\n",
            "Epoch [19/20] Batch 400/1583                   Loss D: 0.3272, loss G: 1.4742\n",
            "Epoch [19/20] Batch 500/1583                   Loss D: 1.1163, loss G: 6.6027\n",
            "Epoch [19/20] Batch 600/1583                   Loss D: 0.1966, loss G: 3.3513\n",
            "Epoch [19/20] Batch 700/1583                   Loss D: 0.6289, loss G: 6.6260\n",
            "Epoch [19/20] Batch 800/1583                   Loss D: 0.1924, loss G: 1.7900\n",
            "Epoch [19/20] Batch 900/1583                   Loss D: 0.1842, loss G: 2.0611\n",
            "Epoch [19/20] Batch 1000/1583                   Loss D: 0.1076, loss G: 3.7606\n",
            "Epoch [19/20] Batch 1100/1583                   Loss D: 0.7512, loss G: 0.7996\n",
            "Epoch [19/20] Batch 1200/1583                   Loss D: 0.2781, loss G: 1.7569\n",
            "Epoch [19/20] Batch 1300/1583                   Loss D: 0.0827, loss G: 4.0122\n",
            "Epoch [19/20] Batch 1400/1583                   Loss D: 0.1084, loss G: 3.7438\n",
            "Epoch [19/20] Batch 1500/1583                   Loss D: 0.1489, loss G: 3.3425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /logs/fake/ /drive/MyDrive/GANS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8avUGseOv2MU",
        "outputId": "f0383957-7971-4694-c675-4a1b3202038c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/logs': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Define la ruta de la carpeta que quieres mover desde Google Colab\n",
        "ruta_carpeta_colab = '/content/logs'\n",
        "\n",
        "# Define la ruta de destino en tu Google Drive\n",
        "ruta_destino_drive = '/content/drive/My Drive/GANS/'\n",
        "\n",
        "# Mueve la carpeta a tu Google Drive\n",
        "shutil.move(ruta_carpeta_colab, ruta_destino_drive)\n"
      ],
      "metadata": {
        "id": "SmXDpRoVrvTY",
        "outputId": "56b2958e-f053-4c23-a5af-fee504817ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/GANS/logs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}