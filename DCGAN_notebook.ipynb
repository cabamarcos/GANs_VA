{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOOwOqd15TQeaDFF0ot6l9t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cabamarcos/GANs_VA/blob/main/DCGAN_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copyfile\n",
        "from google.colab import drive\n",
        "import os, sys\n",
        "# GUARDAR FICEHROS EN DRIVE Y LUEGO MONTAR DRIVE EN COLAB Y PONER EL PATH DONDE ESTAN LOS FICHEROS\n",
        "drive.mount('/content/drive')\n",
        "copyfile('/content/drive/My Drive/GANS/archive.zip', './archive.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "rrwDY_3XlMaZ",
        "outputId": "e549c40f-3017-40dc-84f0-4d9deb1cca00"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./archive.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/My Drive/GANS'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIIrXozdlO4G",
        "outputId": "c10b2800-4673-4478-d36b-1486d95f1529"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/GANS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copy2('archive.zip', '/content/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bS21-CUPnjMm",
        "outputId": "45ae437a-7d56-415d-d086-ed4cf8c7afcd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/archive.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"..\"\n",
        "%cd \"..\"\n",
        "%cd \"..\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOXzFlqAtbIk",
        "outputId": "8dd4d670-5a44-4101-a91e-76efa9f4d780"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n",
            "/content/drive\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset\n",
        "!unzip -q archive.zip\n",
        "# Remove the zip file\n",
        "!rm archive.zip"
      ],
      "metadata": {
        "id": "82O35JSURskZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # input: N x channels_img x 64 x 64\n",
        "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
        "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "def test():\n",
        "    N, in_channels, H, W = 8, 3, 64, 64\n",
        "    noise_dim = 100\n",
        "    x = torch.randn((N, in_channels, H, W))\n",
        "    disc = Discriminator(in_channels, 8)\n",
        "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
        "    gen = Generator(noise_dim, in_channels, 8)\n",
        "    z = torch.randn((N, noise_dim, 1, 1))\n",
        "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
        "    print(\"All tests passed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQlHLNgPnJvN",
        "outputId": "8c3f7b93-d8fd-4093-f899-4f254ba39632"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2prTvVYsmSR8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters etc.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
        "BATCH_SIZE = 128\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 3\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 25\n",
        "FEATURES_DISC = 64\n",
        "FEATURES_GEN = 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU8pMdfvmyXB",
        "outputId": "c4563ca2-0fc3-4ecf-daa4-f87d61d08089"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Z_fm8A-bm3op"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(disc)\n",
        "\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "GT3SWo2ym-Xa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"logs/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/fake\")\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "disc.train()"
      ],
      "metadata": {
        "id": "zCCMsWz9nAsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18ae943-f63f-4f32-b367-08707d6d9775"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (disc): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2)\n",
              "    (2): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (6): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (real, _) in enumerate(dataloader):\n",
        "        real = real.to(device)\n",
        "        noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
        "        fake = gen(noise)\n",
        "\n",
        "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
        "        disc_real = disc(real).reshape(-1)\n",
        "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
        "        disc_fake = disc(fake.detach()).reshape(-1)\n",
        "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
        "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
        "        disc.zero_grad()\n",
        "        loss_disc.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
        "        output = disc(fake).reshape(-1)\n",
        "        loss_gen = criterion(output, torch.ones_like(output))\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
        "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise)\n",
        "                # take out (up to) 32 examples\n",
        "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
        "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
        "\n",
        "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "\n",
        "            step += 1"
      ],
      "metadata": {
        "id": "Dak0bkOInD_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac17f16-840f-4fb7-8ebf-7284f9452d6c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/25] Batch 0/1583                   Loss D: 0.6850, loss G: 0.8059\n",
            "Epoch [0/25] Batch 100/1583                   Loss D: 0.1665, loss G: 3.8176\n",
            "Epoch [0/25] Batch 200/1583                   Loss D: 0.5439, loss G: 2.3592\n",
            "Epoch [0/25] Batch 300/1583                   Loss D: 0.4934, loss G: 1.5815\n",
            "Epoch [0/25] Batch 400/1583                   Loss D: 0.4928, loss G: 1.6528\n",
            "Epoch [0/25] Batch 500/1583                   Loss D: 0.6250, loss G: 1.1078\n",
            "Epoch [0/25] Batch 600/1583                   Loss D: 0.6372, loss G: 1.8254\n",
            "Epoch [0/25] Batch 700/1583                   Loss D: 0.5192, loss G: 1.7026\n",
            "Epoch [0/25] Batch 800/1583                   Loss D: 0.5788, loss G: 2.7486\n",
            "Epoch [0/25] Batch 900/1583                   Loss D: 0.7267, loss G: 2.7745\n",
            "Epoch [0/25] Batch 1000/1583                   Loss D: 0.5697, loss G: 1.9441\n",
            "Epoch [0/25] Batch 1100/1583                   Loss D: 0.6298, loss G: 1.2522\n",
            "Epoch [0/25] Batch 1200/1583                   Loss D: 0.4895, loss G: 1.9370\n",
            "Epoch [0/25] Batch 1300/1583                   Loss D: 0.4609, loss G: 1.7207\n",
            "Epoch [0/25] Batch 1400/1583                   Loss D: 0.6073, loss G: 1.7708\n",
            "Epoch [0/25] Batch 1500/1583                   Loss D: 0.5787, loss G: 2.3679\n",
            "Epoch [1/25] Batch 0/1583                   Loss D: 0.4493, loss G: 1.9275\n",
            "Epoch [1/25] Batch 100/1583                   Loss D: 0.5030, loss G: 2.0640\n",
            "Epoch [1/25] Batch 200/1583                   Loss D: 0.4542, loss G: 1.9564\n",
            "Epoch [1/25] Batch 300/1583                   Loss D: 0.4670, loss G: 2.4091\n",
            "Epoch [1/25] Batch 400/1583                   Loss D: 0.5072, loss G: 1.3995\n",
            "Epoch [1/25] Batch 500/1583                   Loss D: 0.6534, loss G: 0.8497\n",
            "Epoch [1/25] Batch 600/1583                   Loss D: 0.3928, loss G: 2.7605\n",
            "Epoch [1/25] Batch 700/1583                   Loss D: 0.4399, loss G: 2.0285\n",
            "Epoch [1/25] Batch 800/1583                   Loss D: 0.5355, loss G: 3.7294\n",
            "Epoch [1/25] Batch 900/1583                   Loss D: 0.4223, loss G: 2.1233\n",
            "Epoch [1/25] Batch 1000/1583                   Loss D: 0.4614, loss G: 1.8852\n",
            "Epoch [1/25] Batch 1100/1583                   Loss D: 0.3573, loss G: 2.2702\n",
            "Epoch [1/25] Batch 1200/1583                   Loss D: 0.5301, loss G: 2.9462\n",
            "Epoch [1/25] Batch 1300/1583                   Loss D: 0.5548, loss G: 1.8999\n",
            "Epoch [1/25] Batch 1400/1583                   Loss D: 0.4351, loss G: 1.3255\n",
            "Epoch [1/25] Batch 1500/1583                   Loss D: 0.6524, loss G: 2.4053\n",
            "Epoch [2/25] Batch 0/1583                   Loss D: 0.4108, loss G: 1.7376\n",
            "Epoch [2/25] Batch 100/1583                   Loss D: 0.7080, loss G: 0.8795\n",
            "Epoch [2/25] Batch 200/1583                   Loss D: 0.5822, loss G: 1.0409\n",
            "Epoch [2/25] Batch 300/1583                   Loss D: 0.5192, loss G: 0.8571\n",
            "Epoch [2/25] Batch 400/1583                   Loss D: 0.5016, loss G: 0.9419\n",
            "Epoch [2/25] Batch 500/1583                   Loss D: 0.4664, loss G: 1.6801\n",
            "Epoch [2/25] Batch 600/1583                   Loss D: 0.5748, loss G: 2.5863\n",
            "Epoch [2/25] Batch 700/1583                   Loss D: 0.4561, loss G: 1.9377\n",
            "Epoch [2/25] Batch 800/1583                   Loss D: 0.5577, loss G: 1.0226\n",
            "Epoch [2/25] Batch 900/1583                   Loss D: 0.5003, loss G: 1.2487\n",
            "Epoch [2/25] Batch 1000/1583                   Loss D: 0.5846, loss G: 0.9271\n",
            "Epoch [2/25] Batch 1100/1583                   Loss D: 0.4796, loss G: 1.0161\n",
            "Epoch [2/25] Batch 1200/1583                   Loss D: 0.4611, loss G: 1.9004\n",
            "Epoch [2/25] Batch 1300/1583                   Loss D: 0.5119, loss G: 1.1560\n",
            "Epoch [2/25] Batch 1400/1583                   Loss D: 0.4403, loss G: 1.4700\n",
            "Epoch [2/25] Batch 1500/1583                   Loss D: 0.4505, loss G: 2.0013\n",
            "Epoch [3/25] Batch 0/1583                   Loss D: 0.4377, loss G: 1.4254\n",
            "Epoch [3/25] Batch 100/1583                   Loss D: 0.4212, loss G: 2.3298\n",
            "Epoch [3/25] Batch 200/1583                   Loss D: 0.5200, loss G: 1.2642\n",
            "Epoch [3/25] Batch 300/1583                   Loss D: 0.4371, loss G: 1.2757\n",
            "Epoch [3/25] Batch 400/1583                   Loss D: 0.4996, loss G: 1.7077\n",
            "Epoch [3/25] Batch 500/1583                   Loss D: 0.5057, loss G: 1.3034\n",
            "Epoch [3/25] Batch 600/1583                   Loss D: 0.4113, loss G: 1.6380\n",
            "Epoch [3/25] Batch 700/1583                   Loss D: 0.5125, loss G: 1.3021\n",
            "Epoch [3/25] Batch 800/1583                   Loss D: 0.5060, loss G: 2.0608\n",
            "Epoch [3/25] Batch 900/1583                   Loss D: 0.4122, loss G: 1.4143\n",
            "Epoch [3/25] Batch 1000/1583                   Loss D: 0.4061, loss G: 1.8054\n",
            "Epoch [3/25] Batch 1100/1583                   Loss D: 0.4796, loss G: 1.3829\n",
            "Epoch [3/25] Batch 1200/1583                   Loss D: 0.4921, loss G: 0.9002\n",
            "Epoch [3/25] Batch 1300/1583                   Loss D: 0.4815, loss G: 1.0878\n",
            "Epoch [3/25] Batch 1400/1583                   Loss D: 0.5963, loss G: 2.4913\n",
            "Epoch [3/25] Batch 1500/1583                   Loss D: 0.3831, loss G: 2.3190\n",
            "Epoch [4/25] Batch 0/1583                   Loss D: 0.3937, loss G: 1.5201\n",
            "Epoch [4/25] Batch 100/1583                   Loss D: 0.4473, loss G: 1.5099\n",
            "Epoch [4/25] Batch 200/1583                   Loss D: 1.2078, loss G: 3.3005\n",
            "Epoch [4/25] Batch 300/1583                   Loss D: 0.5325, loss G: 2.4395\n",
            "Epoch [4/25] Batch 400/1583                   Loss D: 0.4858, loss G: 1.1764\n",
            "Epoch [4/25] Batch 500/1583                   Loss D: 0.5163, loss G: 2.1566\n",
            "Epoch [4/25] Batch 600/1583                   Loss D: 0.3034, loss G: 2.3532\n",
            "Epoch [4/25] Batch 700/1583                   Loss D: 0.5151, loss G: 1.2729\n",
            "Epoch [4/25] Batch 800/1583                   Loss D: 0.4114, loss G: 1.3172\n",
            "Epoch [4/25] Batch 900/1583                   Loss D: 0.8437, loss G: 0.4052\n",
            "Epoch [4/25] Batch 1000/1583                   Loss D: 0.4244, loss G: 1.6241\n",
            "Epoch [4/25] Batch 1100/1583                   Loss D: 0.6192, loss G: 1.1373\n",
            "Epoch [4/25] Batch 1200/1583                   Loss D: 0.5353, loss G: 2.0353\n",
            "Epoch [4/25] Batch 1300/1583                   Loss D: 0.4327, loss G: 2.1958\n",
            "Epoch [4/25] Batch 1400/1583                   Loss D: 0.5513, loss G: 1.1187\n",
            "Epoch [4/25] Batch 1500/1583                   Loss D: 0.4658, loss G: 1.2946\n",
            "Epoch [5/25] Batch 0/1583                   Loss D: 0.4757, loss G: 2.4434\n",
            "Epoch [5/25] Batch 100/1583                   Loss D: 0.4604, loss G: 0.9348\n",
            "Epoch [5/25] Batch 200/1583                   Loss D: 0.4195, loss G: 1.2548\n",
            "Epoch [5/25] Batch 300/1583                   Loss D: 0.4133, loss G: 1.7621\n",
            "Epoch [5/25] Batch 400/1583                   Loss D: 0.3676, loss G: 2.0715\n",
            "Epoch [5/25] Batch 500/1583                   Loss D: 0.4204, loss G: 1.3747\n",
            "Epoch [5/25] Batch 600/1583                   Loss D: 0.4634, loss G: 2.5699\n",
            "Epoch [5/25] Batch 700/1583                   Loss D: 0.6290, loss G: 1.3471\n",
            "Epoch [5/25] Batch 800/1583                   Loss D: 0.6420, loss G: 3.3771\n",
            "Epoch [5/25] Batch 900/1583                   Loss D: 0.4824, loss G: 2.9302\n",
            "Epoch [5/25] Batch 1000/1583                   Loss D: 0.4103, loss G: 2.3520\n",
            "Epoch [5/25] Batch 1100/1583                   Loss D: 0.3497, loss G: 2.0339\n",
            "Epoch [5/25] Batch 1200/1583                   Loss D: 0.4355, loss G: 2.0388\n",
            "Epoch [5/25] Batch 1300/1583                   Loss D: 0.4528, loss G: 1.1275\n",
            "Epoch [5/25] Batch 1400/1583                   Loss D: 0.5487, loss G: 1.8524\n",
            "Epoch [5/25] Batch 1500/1583                   Loss D: 0.4668, loss G: 0.8677\n",
            "Epoch [6/25] Batch 0/1583                   Loss D: 0.5323, loss G: 3.1225\n",
            "Epoch [6/25] Batch 100/1583                   Loss D: 0.4205, loss G: 0.7880\n",
            "Epoch [6/25] Batch 200/1583                   Loss D: 0.3975, loss G: 3.4276\n",
            "Epoch [6/25] Batch 300/1583                   Loss D: 0.3891, loss G: 2.0972\n",
            "Epoch [6/25] Batch 400/1583                   Loss D: 0.4378, loss G: 1.9800\n",
            "Epoch [6/25] Batch 500/1583                   Loss D: 0.3045, loss G: 2.4993\n",
            "Epoch [6/25] Batch 600/1583                   Loss D: 0.3764, loss G: 2.3304\n",
            "Epoch [6/25] Batch 700/1583                   Loss D: 0.3535, loss G: 1.4070\n",
            "Epoch [6/25] Batch 800/1583                   Loss D: 0.3817, loss G: 1.6910\n",
            "Epoch [6/25] Batch 900/1583                   Loss D: 0.3256, loss G: 1.7657\n",
            "Epoch [6/25] Batch 1000/1583                   Loss D: 0.4317, loss G: 2.5759\n",
            "Epoch [6/25] Batch 1100/1583                   Loss D: 0.3592, loss G: 2.8585\n",
            "Epoch [6/25] Batch 1200/1583                   Loss D: 0.3056, loss G: 1.7771\n",
            "Epoch [6/25] Batch 1300/1583                   Loss D: 0.2866, loss G: 1.4232\n",
            "Epoch [6/25] Batch 1400/1583                   Loss D: 0.3599, loss G: 2.6373\n",
            "Epoch [6/25] Batch 1500/1583                   Loss D: 0.5507, loss G: 1.4169\n",
            "Epoch [7/25] Batch 0/1583                   Loss D: 0.3393, loss G: 2.0965\n",
            "Epoch [7/25] Batch 100/1583                   Loss D: 0.4837, loss G: 3.5730\n",
            "Epoch [7/25] Batch 200/1583                   Loss D: 0.3743, loss G: 2.1459\n",
            "Epoch [7/25] Batch 300/1583                   Loss D: 0.2346, loss G: 1.9940\n",
            "Epoch [7/25] Batch 400/1583                   Loss D: 0.5624, loss G: 0.4525\n",
            "Epoch [7/25] Batch 500/1583                   Loss D: 0.3029, loss G: 2.5390\n",
            "Epoch [7/25] Batch 600/1583                   Loss D: 0.5540, loss G: 0.6905\n",
            "Epoch [7/25] Batch 700/1583                   Loss D: 0.4514, loss G: 2.9997\n",
            "Epoch [7/25] Batch 800/1583                   Loss D: 0.4378, loss G: 3.6274\n",
            "Epoch [7/25] Batch 900/1583                   Loss D: 0.2955, loss G: 1.9361\n",
            "Epoch [7/25] Batch 1000/1583                   Loss D: 0.2609, loss G: 1.7091\n",
            "Epoch [7/25] Batch 1100/1583                   Loss D: 0.3766, loss G: 1.6401\n",
            "Epoch [7/25] Batch 1200/1583                   Loss D: 0.3956, loss G: 1.5994\n",
            "Epoch [7/25] Batch 1300/1583                   Loss D: 0.3211, loss G: 1.5212\n",
            "Epoch [7/25] Batch 1400/1583                   Loss D: 0.3615, loss G: 1.6782\n",
            "Epoch [7/25] Batch 1500/1583                   Loss D: 0.3256, loss G: 1.8237\n",
            "Epoch [8/25] Batch 0/1583                   Loss D: 0.2756, loss G: 2.0240\n",
            "Epoch [8/25] Batch 100/1583                   Loss D: 0.2480, loss G: 2.1903\n",
            "Epoch [8/25] Batch 200/1583                   Loss D: 0.4972, loss G: 2.5040\n",
            "Epoch [8/25] Batch 300/1583                   Loss D: 0.2962, loss G: 2.8121\n",
            "Epoch [8/25] Batch 400/1583                   Loss D: 0.2908, loss G: 2.0050\n",
            "Epoch [8/25] Batch 500/1583                   Loss D: 0.2790, loss G: 2.0459\n",
            "Epoch [8/25] Batch 600/1583                   Loss D: 0.2392, loss G: 2.1098\n",
            "Epoch [8/25] Batch 700/1583                   Loss D: 0.5691, loss G: 0.8615\n",
            "Epoch [8/25] Batch 800/1583                   Loss D: 0.2927, loss G: 2.0426\n",
            "Epoch [8/25] Batch 900/1583                   Loss D: 0.2379, loss G: 2.1435\n",
            "Epoch [8/25] Batch 1000/1583                   Loss D: 0.3303, loss G: 1.6844\n",
            "Epoch [8/25] Batch 1100/1583                   Loss D: 0.3944, loss G: 1.3290\n",
            "Epoch [8/25] Batch 1200/1583                   Loss D: 0.4136, loss G: 1.0268\n",
            "Epoch [8/25] Batch 1300/1583                   Loss D: 0.6906, loss G: 4.2027\n",
            "Epoch [8/25] Batch 1400/1583                   Loss D: 0.3316, loss G: 3.2616\n",
            "Epoch [8/25] Batch 1500/1583                   Loss D: 0.2114, loss G: 2.6579\n",
            "Epoch [9/25] Batch 0/1583                   Loss D: 0.4813, loss G: 0.7692\n",
            "Epoch [9/25] Batch 100/1583                   Loss D: 0.3760, loss G: 4.4430\n",
            "Epoch [9/25] Batch 200/1583                   Loss D: 0.2633, loss G: 2.4034\n",
            "Epoch [9/25] Batch 300/1583                   Loss D: 0.2671, loss G: 2.1616\n",
            "Epoch [9/25] Batch 400/1583                   Loss D: 0.2309, loss G: 2.5449\n",
            "Epoch [9/25] Batch 500/1583                   Loss D: 0.2255, loss G: 2.7636\n",
            "Epoch [9/25] Batch 600/1583                   Loss D: 0.2828, loss G: 3.8321\n",
            "Epoch [9/25] Batch 700/1583                   Loss D: 0.3688, loss G: 4.5199\n",
            "Epoch [9/25] Batch 800/1583                   Loss D: 0.3800, loss G: 1.4013\n",
            "Epoch [9/25] Batch 900/1583                   Loss D: 0.2358, loss G: 2.6288\n",
            "Epoch [9/25] Batch 1000/1583                   Loss D: 0.2626, loss G: 2.7814\n",
            "Epoch [9/25] Batch 1100/1583                   Loss D: 0.2877, loss G: 3.2090\n",
            "Epoch [9/25] Batch 1200/1583                   Loss D: 0.2141, loss G: 3.1252\n",
            "Epoch [9/25] Batch 1300/1583                   Loss D: 0.3257, loss G: 1.7712\n",
            "Epoch [9/25] Batch 1400/1583                   Loss D: 0.1875, loss G: 2.9032\n",
            "Epoch [9/25] Batch 1500/1583                   Loss D: 0.3579, loss G: 2.9992\n",
            "Epoch [10/25] Batch 0/1583                   Loss D: 0.2827, loss G: 2.1704\n",
            "Epoch [10/25] Batch 100/1583                   Loss D: 0.2428, loss G: 3.1619\n",
            "Epoch [10/25] Batch 200/1583                   Loss D: 0.4576, loss G: 1.3500\n",
            "Epoch [10/25] Batch 300/1583                   Loss D: 0.1847, loss G: 2.9363\n",
            "Epoch [10/25] Batch 400/1583                   Loss D: 0.1912, loss G: 3.1499\n",
            "Epoch [10/25] Batch 500/1583                   Loss D: 0.2225, loss G: 2.0634\n",
            "Epoch [10/25] Batch 600/1583                   Loss D: 0.2018, loss G: 3.0769\n",
            "Epoch [10/25] Batch 700/1583                   Loss D: 0.2662, loss G: 2.1103\n",
            "Epoch [10/25] Batch 800/1583                   Loss D: 0.2755, loss G: 3.3831\n",
            "Epoch [10/25] Batch 900/1583                   Loss D: 0.2021, loss G: 2.4126\n",
            "Epoch [10/25] Batch 1000/1583                   Loss D: 0.4039, loss G: 1.3813\n",
            "Epoch [10/25] Batch 1100/1583                   Loss D: 0.1969, loss G: 2.4759\n",
            "Epoch [10/25] Batch 1200/1583                   Loss D: 0.1996, loss G: 3.0081\n",
            "Epoch [10/25] Batch 1300/1583                   Loss D: 0.3414, loss G: 1.3681\n",
            "Epoch [10/25] Batch 1400/1583                   Loss D: 0.2242, loss G: 1.7184\n",
            "Epoch [10/25] Batch 1500/1583                   Loss D: 0.1485, loss G: 2.7933\n",
            "Epoch [11/25] Batch 0/1583                   Loss D: 0.2591, loss G: 3.1833\n",
            "Epoch [11/25] Batch 100/1583                   Loss D: 0.3785, loss G: 1.0556\n",
            "Epoch [11/25] Batch 200/1583                   Loss D: 0.3073, loss G: 1.4213\n",
            "Epoch [11/25] Batch 300/1583                   Loss D: 0.1608, loss G: 2.9186\n",
            "Epoch [11/25] Batch 400/1583                   Loss D: 0.1358, loss G: 3.3971\n",
            "Epoch [11/25] Batch 500/1583                   Loss D: 0.2509, loss G: 2.9974\n",
            "Epoch [11/25] Batch 600/1583                   Loss D: 0.2002, loss G: 2.6558\n",
            "Epoch [11/25] Batch 700/1583                   Loss D: 0.1785, loss G: 2.2249\n",
            "Epoch [11/25] Batch 800/1583                   Loss D: 0.4050, loss G: 2.0134\n",
            "Epoch [11/25] Batch 900/1583                   Loss D: 0.2881, loss G: 3.7785\n",
            "Epoch [11/25] Batch 1000/1583                   Loss D: 0.2465, loss G: 1.8536\n",
            "Epoch [11/25] Batch 1100/1583                   Loss D: 0.2217, loss G: 1.8574\n",
            "Epoch [11/25] Batch 1200/1583                   Loss D: 0.1235, loss G: 3.2466\n",
            "Epoch [11/25] Batch 1300/1583                   Loss D: 0.3222, loss G: 2.2032\n",
            "Epoch [11/25] Batch 1400/1583                   Loss D: 0.2132, loss G: 2.1075\n",
            "Epoch [11/25] Batch 1500/1583                   Loss D: 0.4538, loss G: 1.1192\n",
            "Epoch [12/25] Batch 0/1583                   Loss D: 0.1865, loss G: 2.6913\n",
            "Epoch [12/25] Batch 100/1583                   Loss D: 0.1665, loss G: 2.6235\n",
            "Epoch [12/25] Batch 200/1583                   Loss D: 0.4355, loss G: 4.1111\n",
            "Epoch [12/25] Batch 300/1583                   Loss D: 0.2062, loss G: 2.1902\n",
            "Epoch [12/25] Batch 400/1583                   Loss D: 0.2709, loss G: 3.8444\n",
            "Epoch [12/25] Batch 500/1583                   Loss D: 0.2371, loss G: 3.3649\n",
            "Epoch [12/25] Batch 600/1583                   Loss D: 0.7356, loss G: 0.4108\n",
            "Epoch [12/25] Batch 700/1583                   Loss D: 0.2355, loss G: 1.9887\n",
            "Epoch [12/25] Batch 800/1583                   Loss D: 0.4364, loss G: 5.1777\n",
            "Epoch [12/25] Batch 900/1583                   Loss D: 0.2146, loss G: 3.1056\n",
            "Epoch [12/25] Batch 1000/1583                   Loss D: 0.2519, loss G: 2.4743\n",
            "Epoch [12/25] Batch 1100/1583                   Loss D: 0.1666, loss G: 2.8334\n",
            "Epoch [12/25] Batch 1200/1583                   Loss D: 0.2287, loss G: 2.1671\n",
            "Epoch [12/25] Batch 1300/1583                   Loss D: 0.7808, loss G: 5.1016\n",
            "Epoch [12/25] Batch 1400/1583                   Loss D: 0.1936, loss G: 3.4650\n",
            "Epoch [12/25] Batch 1500/1583                   Loss D: 0.3212, loss G: 2.3206\n",
            "Epoch [13/25] Batch 0/1583                   Loss D: 0.4080, loss G: 0.9393\n",
            "Epoch [13/25] Batch 100/1583                   Loss D: 0.2108, loss G: 2.0830\n",
            "Epoch [13/25] Batch 200/1583                   Loss D: 0.2411, loss G: 4.1878\n",
            "Epoch [13/25] Batch 300/1583                   Loss D: 0.4255, loss G: 3.8098\n",
            "Epoch [13/25] Batch 400/1583                   Loss D: 0.2589, loss G: 4.0834\n",
            "Epoch [13/25] Batch 500/1583                   Loss D: 0.1883, loss G: 3.5784\n",
            "Epoch [13/25] Batch 600/1583                   Loss D: 0.2042, loss G: 4.2618\n",
            "Epoch [13/25] Batch 700/1583                   Loss D: 0.2679, loss G: 4.0804\n",
            "Epoch [13/25] Batch 800/1583                   Loss D: 0.7079, loss G: 0.5425\n",
            "Epoch [13/25] Batch 900/1583                   Loss D: 0.1564, loss G: 2.7305\n",
            "Epoch [13/25] Batch 1000/1583                   Loss D: 0.1403, loss G: 3.2967\n",
            "Epoch [13/25] Batch 1100/1583                   Loss D: 0.1849, loss G: 2.8120\n",
            "Epoch [13/25] Batch 1200/1583                   Loss D: 0.1073, loss G: 3.3739\n",
            "Epoch [13/25] Batch 1300/1583                   Loss D: 0.2612, loss G: 5.1912\n",
            "Epoch [13/25] Batch 1400/1583                   Loss D: 0.2091, loss G: 3.1180\n",
            "Epoch [13/25] Batch 1500/1583                   Loss D: 0.2312, loss G: 2.2925\n",
            "Epoch [14/25] Batch 0/1583                   Loss D: 0.2207, loss G: 2.3281\n",
            "Epoch [14/25] Batch 100/1583                   Loss D: 0.2221, loss G: 2.0908\n",
            "Epoch [14/25] Batch 200/1583                   Loss D: 0.1191, loss G: 3.0650\n",
            "Epoch [14/25] Batch 300/1583                   Loss D: 0.6489, loss G: 1.1700\n",
            "Epoch [14/25] Batch 400/1583                   Loss D: 0.1240, loss G: 3.0554\n",
            "Epoch [14/25] Batch 500/1583                   Loss D: 0.1087, loss G: 2.9502\n",
            "Epoch [14/25] Batch 600/1583                   Loss D: 0.1188, loss G: 3.5016\n",
            "Epoch [14/25] Batch 700/1583                   Loss D: 0.1261, loss G: 2.9115\n",
            "Epoch [14/25] Batch 800/1583                   Loss D: 0.2168, loss G: 2.3271\n",
            "Epoch [14/25] Batch 900/1583                   Loss D: 0.3103, loss G: 1.3251\n",
            "Epoch [14/25] Batch 1000/1583                   Loss D: 0.1641, loss G: 3.1220\n",
            "Epoch [14/25] Batch 1100/1583                   Loss D: 0.0767, loss G: 4.0364\n",
            "Epoch [14/25] Batch 1200/1583                   Loss D: 0.1133, loss G: 3.5354\n",
            "Epoch [14/25] Batch 1300/1583                   Loss D: 0.2965, loss G: 3.8194\n",
            "Epoch [14/25] Batch 1400/1583                   Loss D: 0.1945, loss G: 1.7395\n",
            "Epoch [14/25] Batch 1500/1583                   Loss D: 0.2216, loss G: 4.5036\n",
            "Epoch [15/25] Batch 0/1583                   Loss D: 0.1222, loss G: 3.0312\n",
            "Epoch [15/25] Batch 100/1583                   Loss D: 0.1057, loss G: 3.7161\n",
            "Epoch [15/25] Batch 200/1583                   Loss D: 0.1930, loss G: 1.8451\n",
            "Epoch [15/25] Batch 300/1583                   Loss D: 0.0751, loss G: 3.5014\n",
            "Epoch [15/25] Batch 400/1583                   Loss D: 0.6052, loss G: 2.7562\n",
            "Epoch [15/25] Batch 500/1583                   Loss D: 0.1330, loss G: 2.6500\n",
            "Epoch [15/25] Batch 600/1583                   Loss D: 0.1480, loss G: 2.1066\n",
            "Epoch [15/25] Batch 700/1583                   Loss D: 0.1105, loss G: 2.7736\n",
            "Epoch [15/25] Batch 800/1583                   Loss D: 2.2265, loss G: 6.2502\n",
            "Epoch [15/25] Batch 900/1583                   Loss D: 0.2278, loss G: 2.2000\n",
            "Epoch [15/25] Batch 1000/1583                   Loss D: 0.1203, loss G: 2.7842\n",
            "Epoch [15/25] Batch 1100/1583                   Loss D: 0.2215, loss G: 4.7493\n",
            "Epoch [15/25] Batch 1200/1583                   Loss D: 0.2002, loss G: 1.6056\n",
            "Epoch [15/25] Batch 1300/1583                   Loss D: 0.3141, loss G: 1.4260\n",
            "Epoch [15/25] Batch 1400/1583                   Loss D: 0.1536, loss G: 2.9036\n",
            "Epoch [15/25] Batch 1500/1583                   Loss D: 0.1892, loss G: 2.4687\n",
            "Epoch [16/25] Batch 0/1583                   Loss D: 0.1189, loss G: 3.8334\n",
            "Epoch [16/25] Batch 100/1583                   Loss D: 0.1298, loss G: 3.8670\n",
            "Epoch [16/25] Batch 200/1583                   Loss D: 0.1574, loss G: 3.6722\n",
            "Epoch [16/25] Batch 300/1583                   Loss D: 0.0701, loss G: 3.5854\n",
            "Epoch [16/25] Batch 400/1583                   Loss D: 0.1025, loss G: 3.5325\n",
            "Epoch [16/25] Batch 500/1583                   Loss D: 0.0837, loss G: 3.7404\n",
            "Epoch [16/25] Batch 600/1583                   Loss D: 0.2436, loss G: 1.4183\n",
            "Epoch [16/25] Batch 700/1583                   Loss D: 0.1891, loss G: 2.3569\n",
            "Epoch [16/25] Batch 800/1583                   Loss D: 0.0767, loss G: 3.4817\n",
            "Epoch [16/25] Batch 900/1583                   Loss D: 0.1642, loss G: 2.3238\n",
            "Epoch [16/25] Batch 1000/1583                   Loss D: 0.0932, loss G: 4.0655\n",
            "Epoch [16/25] Batch 1100/1583                   Loss D: 0.1103, loss G: 2.7205\n",
            "Epoch [16/25] Batch 1200/1583                   Loss D: 0.0978, loss G: 3.9283\n",
            "Epoch [16/25] Batch 1300/1583                   Loss D: 0.1252, loss G: 2.7154\n",
            "Epoch [16/25] Batch 1400/1583                   Loss D: 0.1471, loss G: 3.0451\n",
            "Epoch [16/25] Batch 1500/1583                   Loss D: 0.0728, loss G: 3.3117\n",
            "Epoch [17/25] Batch 0/1583                   Loss D: 0.3404, loss G: 2.1286\n",
            "Epoch [17/25] Batch 100/1583                   Loss D: 0.1685, loss G: 4.0105\n",
            "Epoch [17/25] Batch 200/1583                   Loss D: 0.0754, loss G: 4.0379\n",
            "Epoch [17/25] Batch 300/1583                   Loss D: 0.1571, loss G: 1.9168\n",
            "Epoch [17/25] Batch 400/1583                   Loss D: 0.1253, loss G: 3.0342\n",
            "Epoch [17/25] Batch 500/1583                   Loss D: 0.4789, loss G: 1.6466\n",
            "Epoch [17/25] Batch 600/1583                   Loss D: 0.1255, loss G: 3.5949\n",
            "Epoch [17/25] Batch 700/1583                   Loss D: 0.0755, loss G: 3.9202\n",
            "Epoch [17/25] Batch 800/1583                   Loss D: 0.2508, loss G: 2.3710\n",
            "Epoch [17/25] Batch 900/1583                   Loss D: 0.1235, loss G: 4.1436\n",
            "Epoch [17/25] Batch 1000/1583                   Loss D: 0.0628, loss G: 3.5852\n",
            "Epoch [17/25] Batch 1100/1583                   Loss D: 0.1034, loss G: 3.9540\n",
            "Epoch [17/25] Batch 1200/1583                   Loss D: 0.1070, loss G: 2.9206\n",
            "Epoch [17/25] Batch 1300/1583                   Loss D: 0.0798, loss G: 3.8170\n",
            "Epoch [17/25] Batch 1400/1583                   Loss D: 0.1082, loss G: 3.5200\n",
            "Epoch [17/25] Batch 1500/1583                   Loss D: 0.0624, loss G: 4.1609\n",
            "Epoch [18/25] Batch 0/1583                   Loss D: 0.3021, loss G: 2.2858\n",
            "Epoch [18/25] Batch 100/1583                   Loss D: 0.7915, loss G: 0.6103\n",
            "Epoch [18/25] Batch 200/1583                   Loss D: 0.4559, loss G: 6.7421\n",
            "Epoch [18/25] Batch 300/1583                   Loss D: 0.1103, loss G: 3.4962\n",
            "Epoch [18/25] Batch 400/1583                   Loss D: 0.1910, loss G: 5.7422\n",
            "Epoch [18/25] Batch 500/1583                   Loss D: 0.0920, loss G: 4.1308\n",
            "Epoch [18/25] Batch 600/1583                   Loss D: 0.0909, loss G: 4.0491\n",
            "Epoch [18/25] Batch 700/1583                   Loss D: 0.4114, loss G: 2.3121\n",
            "Epoch [18/25] Batch 800/1583                   Loss D: 0.1279, loss G: 3.7969\n",
            "Epoch [18/25] Batch 900/1583                   Loss D: 0.1423, loss G: 3.3167\n",
            "Epoch [18/25] Batch 1000/1583                   Loss D: 0.4110, loss G: 1.3654\n",
            "Epoch [18/25] Batch 1100/1583                   Loss D: 0.1758, loss G: 3.4239\n",
            "Epoch [18/25] Batch 1200/1583                   Loss D: 0.0645, loss G: 3.5936\n",
            "Epoch [18/25] Batch 1300/1583                   Loss D: 0.2881, loss G: 4.5548\n",
            "Epoch [18/25] Batch 1400/1583                   Loss D: 0.0654, loss G: 3.5983\n",
            "Epoch [18/25] Batch 1500/1583                   Loss D: 0.0987, loss G: 3.4878\n",
            "Epoch [19/25] Batch 0/1583                   Loss D: 0.2044, loss G: 5.4526\n",
            "Epoch [19/25] Batch 100/1583                   Loss D: 0.1845, loss G: 5.5742\n",
            "Epoch [19/25] Batch 200/1583                   Loss D: 0.2395, loss G: 2.2253\n",
            "Epoch [19/25] Batch 300/1583                   Loss D: 0.2084, loss G: 1.8052\n",
            "Epoch [19/25] Batch 400/1583                   Loss D: 0.1557, loss G: 4.2935\n",
            "Epoch [19/25] Batch 500/1583                   Loss D: 0.0633, loss G: 4.4659\n",
            "Epoch [19/25] Batch 600/1583                   Loss D: 0.0485, loss G: 4.4855\n",
            "Epoch [19/25] Batch 700/1583                   Loss D: 0.1985, loss G: 7.1170\n",
            "Epoch [19/25] Batch 800/1583                   Loss D: 0.6613, loss G: 0.8990\n",
            "Epoch [19/25] Batch 900/1583                   Loss D: 0.0728, loss G: 4.3897\n",
            "Epoch [19/25] Batch 1000/1583                   Loss D: 0.0434, loss G: 3.8138\n",
            "Epoch [19/25] Batch 1100/1583                   Loss D: 0.1093, loss G: 4.8839\n",
            "Epoch [19/25] Batch 1200/1583                   Loss D: 0.1038, loss G: 4.9968\n",
            "Epoch [19/25] Batch 1300/1583                   Loss D: 0.0945, loss G: 4.6724\n",
            "Epoch [19/25] Batch 1400/1583                   Loss D: 0.4198, loss G: 3.2644\n",
            "Epoch [19/25] Batch 1500/1583                   Loss D: 0.0956, loss G: 4.0234\n",
            "Epoch [20/25] Batch 0/1583                   Loss D: 0.3413, loss G: 5.0892\n",
            "Epoch [20/25] Batch 100/1583                   Loss D: 0.0840, loss G: 3.2921\n",
            "Epoch [20/25] Batch 200/1583                   Loss D: 0.0820, loss G: 3.1771\n",
            "Epoch [20/25] Batch 300/1583                   Loss D: 0.0965, loss G: 3.2372\n",
            "Epoch [20/25] Batch 400/1583                   Loss D: 0.1405, loss G: 2.4648\n",
            "Epoch [20/25] Batch 500/1583                   Loss D: 0.0977, loss G: 3.2149\n",
            "Epoch [20/25] Batch 600/1583                   Loss D: 0.0657, loss G: 4.5291\n",
            "Epoch [20/25] Batch 700/1583                   Loss D: 0.1166, loss G: 4.1067\n",
            "Epoch [20/25] Batch 800/1583                   Loss D: 0.0755, loss G: 3.8610\n",
            "Epoch [20/25] Batch 900/1583                   Loss D: 0.0730, loss G: 3.5051\n",
            "Epoch [20/25] Batch 1000/1583                   Loss D: 0.0751, loss G: 3.2619\n",
            "Epoch [20/25] Batch 1100/1583                   Loss D: 0.4093, loss G: 4.6672\n",
            "Epoch [20/25] Batch 1200/1583                   Loss D: 0.3071, loss G: 3.3772\n",
            "Epoch [20/25] Batch 1300/1583                   Loss D: 0.1879, loss G: 2.2596\n",
            "Epoch [20/25] Batch 1400/1583                   Loss D: 0.1434, loss G: 2.4219\n",
            "Epoch [20/25] Batch 1500/1583                   Loss D: 0.1799, loss G: 2.9769\n",
            "Epoch [21/25] Batch 0/1583                   Loss D: 0.3754, loss G: 7.1542\n",
            "Epoch [21/25] Batch 100/1583                   Loss D: 0.0429, loss G: 4.9702\n",
            "Epoch [21/25] Batch 200/1583                   Loss D: 0.1670, loss G: 2.8769\n",
            "Epoch [21/25] Batch 300/1583                   Loss D: 0.1094, loss G: 3.4018\n",
            "Epoch [21/25] Batch 400/1583                   Loss D: 0.0883, loss G: 3.2637\n",
            "Epoch [21/25] Batch 500/1583                   Loss D: 0.2562, loss G: 5.2421\n",
            "Epoch [21/25] Batch 600/1583                   Loss D: 0.1645, loss G: 3.5126\n",
            "Epoch [21/25] Batch 700/1583                   Loss D: 0.1511, loss G: 2.8739\n",
            "Epoch [21/25] Batch 800/1583                   Loss D: 0.1782, loss G: 6.6063\n",
            "Epoch [21/25] Batch 900/1583                   Loss D: 0.0891, loss G: 4.3558\n",
            "Epoch [21/25] Batch 1000/1583                   Loss D: 0.0548, loss G: 3.6873\n",
            "Epoch [21/25] Batch 1100/1583                   Loss D: 0.1979, loss G: 2.8738\n",
            "Epoch [21/25] Batch 1200/1583                   Loss D: 0.0623, loss G: 4.3800\n",
            "Epoch [21/25] Batch 1300/1583                   Loss D: 0.1126, loss G: 3.2490\n",
            "Epoch [21/25] Batch 1400/1583                   Loss D: 0.1905, loss G: 2.0605\n",
            "Epoch [21/25] Batch 1500/1583                   Loss D: 0.0711, loss G: 3.8042\n",
            "Epoch [22/25] Batch 0/1583                   Loss D: 0.1077, loss G: 3.9774\n",
            "Epoch [22/25] Batch 100/1583                   Loss D: 0.0638, loss G: 5.0023\n",
            "Epoch [22/25] Batch 200/1583                   Loss D: 0.0953, loss G: 4.6428\n",
            "Epoch [22/25] Batch 300/1583                   Loss D: 0.1382, loss G: 3.7448\n",
            "Epoch [22/25] Batch 400/1583                   Loss D: 0.2520, loss G: 1.9762\n",
            "Epoch [22/25] Batch 500/1583                   Loss D: 0.1526, loss G: 6.0800\n",
            "Epoch [22/25] Batch 600/1583                   Loss D: 0.1565, loss G: 5.3065\n",
            "Epoch [22/25] Batch 700/1583                   Loss D: 0.2548, loss G: 5.3182\n",
            "Epoch [22/25] Batch 800/1583                   Loss D: 0.2487, loss G: 1.2013\n",
            "Epoch [22/25] Batch 900/1583                   Loss D: 0.6068, loss G: 5.9728\n",
            "Epoch [22/25] Batch 1000/1583                   Loss D: 0.0670, loss G: 3.6602\n",
            "Epoch [22/25] Batch 1100/1583                   Loss D: 0.1309, loss G: 2.9573\n",
            "Epoch [22/25] Batch 1200/1583                   Loss D: 0.0634, loss G: 3.8683\n",
            "Epoch [22/25] Batch 1300/1583                   Loss D: 0.1925, loss G: 3.4577\n",
            "Epoch [22/25] Batch 1400/1583                   Loss D: 0.0772, loss G: 4.7128\n",
            "Epoch [22/25] Batch 1500/1583                   Loss D: 0.2032, loss G: 2.3205\n",
            "Epoch [23/25] Batch 0/1583                   Loss D: 0.1310, loss G: 2.6009\n",
            "Epoch [23/25] Batch 100/1583                   Loss D: 0.0353, loss G: 5.2756\n",
            "Epoch [23/25] Batch 200/1583                   Loss D: 0.0414, loss G: 4.3087\n",
            "Epoch [23/25] Batch 300/1583                   Loss D: 0.0972, loss G: 4.3238\n",
            "Epoch [23/25] Batch 400/1583                   Loss D: 0.1966, loss G: 2.4229\n",
            "Epoch [23/25] Batch 500/1583                   Loss D: 0.1967, loss G: 5.0324\n",
            "Epoch [23/25] Batch 600/1583                   Loss D: 0.1077, loss G: 3.0405\n",
            "Epoch [23/25] Batch 700/1583                   Loss D: 0.2611, loss G: 5.1185\n",
            "Epoch [23/25] Batch 800/1583                   Loss D: 0.3108, loss G: 2.2716\n",
            "Epoch [23/25] Batch 900/1583                   Loss D: 0.0609, loss G: 4.8603\n",
            "Epoch [23/25] Batch 1000/1583                   Loss D: 7.2030, loss G: 2.7151\n",
            "Epoch [23/25] Batch 1100/1583                   Loss D: 0.0978, loss G: 3.4149\n",
            "Epoch [23/25] Batch 1200/1583                   Loss D: 0.0737, loss G: 4.2769\n",
            "Epoch [23/25] Batch 1300/1583                   Loss D: 0.1073, loss G: 3.6081\n",
            "Epoch [23/25] Batch 1400/1583                   Loss D: 0.0378, loss G: 4.3597\n",
            "Epoch [23/25] Batch 1500/1583                   Loss D: 0.0528, loss G: 3.7294\n",
            "Epoch [24/25] Batch 0/1583                   Loss D: 0.1302, loss G: 4.3680\n",
            "Epoch [24/25] Batch 100/1583                   Loss D: 0.0336, loss G: 4.1286\n",
            "Epoch [24/25] Batch 200/1583                   Loss D: 0.3810, loss G: 9.4760\n",
            "Epoch [24/25] Batch 300/1583                   Loss D: 0.0663, loss G: 4.1197\n",
            "Epoch [24/25] Batch 400/1583                   Loss D: 0.0837, loss G: 3.7498\n",
            "Epoch [24/25] Batch 500/1583                   Loss D: 0.0791, loss G: 4.8169\n",
            "Epoch [24/25] Batch 600/1583                   Loss D: 0.0832, loss G: 2.5896\n",
            "Epoch [24/25] Batch 700/1583                   Loss D: 0.1575, loss G: 3.2682\n",
            "Epoch [24/25] Batch 800/1583                   Loss D: 0.4989, loss G: 11.8682\n",
            "Epoch [24/25] Batch 900/1583                   Loss D: 0.1896, loss G: 3.2062\n",
            "Epoch [24/25] Batch 1000/1583                   Loss D: 0.0974, loss G: 3.1956\n",
            "Epoch [24/25] Batch 1100/1583                   Loss D: 0.0625, loss G: 3.2265\n",
            "Epoch [24/25] Batch 1200/1583                   Loss D: 0.2326, loss G: 2.9541\n",
            "Epoch [24/25] Batch 1300/1583                   Loss D: 0.0873, loss G: 2.6891\n",
            "Epoch [24/25] Batch 1400/1583                   Loss D: 0.1676, loss G: 4.0282\n",
            "Epoch [24/25] Batch 1500/1583                   Loss D: 0.1379, loss G: 2.8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /logs /drive/MyDrive/GANS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8avUGseOv2MU",
        "outputId": "865e824b-50c7-46e9-fe67-332e731ea7f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/logs': No such file or directory\n"
          ]
        }
      ]
    }
  ]
}