{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cabamarcos/GANs_VA/blob/main/WGAN_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv_ntdaq42PD"
      },
      "outputs": [],
      "source": [
        "from shutil import copyfile\n",
        "from google.colab import drive\n",
        "import os, sys\n",
        "# GUARDAR FICEHROS EN DRIVE Y LUEGO MONTAR DRIVE EN COLAB Y PONER EL PATH DONDE ESTAN LOS FICHEROS\n",
        "drive.mount('/content/drive')\n",
        "copyfile('/content/drive/My Drive/GANS/archive.zip', './archive.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shSxLzUZ48Km"
      },
      "outputs": [],
      "source": [
        "%cd '/content/drive/My Drive/GANS'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKpkBovh4-L2"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.copy2('archive.zip', '/content/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Rvnj0fN5Aou"
      },
      "outputs": [],
      "source": [
        "%cd \"..\"\n",
        "%cd \"..\"\n",
        "%cd \"..\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFfPzmQB5B8u"
      },
      "outputs": [],
      "source": [
        "# Unzip the dataset\n",
        "!unzip -q archive.zip\n",
        "# Remove the zip file\n",
        "!rm archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W--zJk8x5GeR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # input: N x channels_img x 64 x 64\n",
        "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
        "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "\n",
        "def test():\n",
        "    N, in_channels, H, W = 8, 3, 64, 64\n",
        "    noise_dim = 100\n",
        "    x = torch.randn((N, in_channels, H, W))\n",
        "    disc = Discriminator(in_channels, 8)\n",
        "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
        "    gen = Generator(noise_dim, in_channels, 8)\n",
        "    z = torch.randn((N, noise_dim, 1, 1))\n",
        "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
        "\n",
        "\n",
        "# test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unQhzv6T5KYU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0-CMZ_05O8_"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters etc.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "LEARNING_RATE = 5e-5\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 3\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS =25\n",
        "FEATURES_CRITIC = 64\n",
        "FEATURES_GEN = 64\n",
        "CRITIC_ITERATIONS = 5\n",
        "WEIGHT_CLIP = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkGzf6nA5R95"
      },
      "outputs": [],
      "source": [
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUp_4Wv75U0x"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize gen and disc/critic\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCGNC1CJ5XVM"
      },
      "outputs": [],
      "source": [
        "# initializate optimizer\n",
        "opt_gen = optim.RMSprop(gen.parameters(), lr=LEARNING_RATE)\n",
        "opt_critic = optim.RMSprop(critic.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# for tensorboard plotting\n",
        "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"logs_WGAN/real\")\n",
        "writer_fake = SummaryWriter(f\"logs_WGAN/fake\")\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "critic.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssGIRxod5Zps"
      },
      "outputs": [],
      "source": [
        "losses_critic = []\n",
        "losses_gen = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (data, _) in enumerate(tqdm(loader)):\n",
        "        data = data.to(device)\n",
        "\n",
        "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
        "        for _ in range(CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
        "            fake = gen(noise)\n",
        "            critic_real = critic(data).reshape(-1)\n",
        "            critic_fake = critic(fake).reshape(-1)\n",
        "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "            # clip critic weights between -0.01, 0.01\n",
        "            for p in critic.parameters():\n",
        "                p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n",
        "\n",
        "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
        "        gen_fake = critic(fake).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Append losses to the lists\n",
        "        losses_critic.append(loss_critic.item())\n",
        "        losses_gen.append(loss_gen.item())\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
        "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise)\n",
        "                # take out (up to) 32 examples\n",
        "                img_grid_real = torchvision.utils.make_grid(\n",
        "                    data[:32], normalize=True\n",
        "                )\n",
        "                img_grid_fake = torchvision.utils.make_grid(\n",
        "                    fake[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "\n",
        "            step += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the losses\n",
        "plt.plot(losses_critic, label='Critic Loss')\n",
        "plt.plot(losses_gen, label='Generator Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('GAN Training Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Define la ruta de la carpeta que quieres mover desde Google Colab\n",
        "ruta_carpeta_colab = '/content/logs_DCGAN_MNIST'\n",
        "\n",
        "# Define la ruta de destino en tu Google Drive\n",
        "ruta_destino_drive = '/content/drive/My Drive/GANS/'\n",
        "\n",
        "# Mueve la carpeta a tu Google Drive\n",
        "shutil.move(ruta_carpeta_colab, ruta_destino_drive)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNuBmyHPvXsc39Mv4raRjUN",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
